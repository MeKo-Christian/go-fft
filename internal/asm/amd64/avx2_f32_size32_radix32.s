//go:build amd64 && asm && !purego

#include "textflag.h"

// ===========================================================================
// Forward transform, size 32, complex64, radix-32 (4x8) variant (AVX2 XMM)
// ===========================================================================
TEXT ·ForwardAVX2Size32Radix32Complex64Asm(SB), NOSPLIT, $0-121
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ bitrev+96(FP), R12
	MOVQ src+32(FP), R13

	CMPQ R13, $32
	JNE  fwd_ret_false

	VMOVUPS ·maskNegLoPS(SB), X15

	// Column pairs 0 & 1
	VMOVUPS 0(R9), X0
	VMOVUPS 64(R9), X1
	VMOVUPS 128(R9), X2
	VMOVUPS 192(R9), X3

	// FFT4 on Columns
	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7

	VMOVAPS X4, X0
	VADDPS  X5, X0, X0
	VMOVAPS X4, X2
	VSUBPS  X5, X2, X2

	VMOVAPS X7, X8
	VSHUFPS $0xB1, X8, X8, X8
	VXORPS  X15, X8, X8

	VMOVAPS X6, X1
	VSUBPS  X8, X1, X1
	VMOVAPS X6, X3
	VADDPS  X8, X3, X3

	// Twiddles for col 0, 1
	VMOVUPS 0(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X1, X11
	VMULPS  X9, X11, X11
	VMOVAPS X1, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X1

	// Row 2: W(32, 2*0), W(32, 2*1)
	MOVSD  0(R10), X8
	MOVHPS 16(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X2, X11
	VMULPS  X9, X11, X11
	VMOVAPS X2, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X2

	// Row 3: W(32, 3*0), W(32, 3*1)
	MOVSD  0(R10), X8
	MOVHPS 24(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X3, X11
	VMULPS  X9, X11, X11
	VMOVAPS X3, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X3

	VMOVUPS X0, 0(R11)
	VMOVUPS X1, 64(R11)
	VMOVUPS X2, 128(R11)
	VMOVUPS X3, 192(R11)

	// Column pairs 2 & 3
	VMOVUPS 16(R9), X0
	VMOVUPS 80(R9), X1
	VMOVUPS 144(R9), X2
	VMOVUPS 208(R9), X3

	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7
	VMOVAPS X4, X0
	VADDPS  X5, X0, X0
	VMOVAPS X4, X2
	VSUBPS  X5, X2, X2
	VMOVAPS X7, X8
	VSHUFPS $0xB1, X8, X8, X8
	VXORPS  X15, X8, X8
	VMOVAPS X6, X1
	VSUBPS  X8, X1, X1
	VMOVAPS X6, X3
	VADDPS  X8, X3, X3

	// Twiddles for col 2, 3
	VMOVUPS 16(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X1, X11
	VMULPS  X9, X11, X11
	VMOVAPS X1, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X1

	MOVSD  32(R10), X8
	MOVHPS 48(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X2, X11
	VMULPS  X9, X11, X11
	VMOVAPS X2, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X2

	MOVSD  48(R10), X8
	MOVHPS 72(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X3, X11
	VMULPS  X9, X11, X11
	VMOVAPS X3, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X3

	VMOVUPS X0, 16(R11)
	VMOVUPS X1, 80(R11)
	VMOVUPS X2, 144(R11)
	VMOVUPS X3, 208(R11)

	// Column pairs 4 & 5
	VMOVUPS 32(R9), X0
	VMOVUPS 96(R9), X1
	VMOVUPS 160(R9), X2
	VMOVUPS 224(R9), X3

	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7
	VMOVAPS X4, X0
	VADDPS  X5, X0, X0
	VMOVAPS X4, X2
	VSUBPS  X5, X2, X2
	VMOVAPS X7, X8
	VSHUFPS $0xB1, X8, X8, X8
	VXORPS  X15, X8, X8
	VMOVAPS X6, X1
	VSUBPS  X8, X1, X1
	VMOVAPS X6, X3
	VADDPS  X8, X3, X3

	// Twiddles for col 4, 5
	VMOVUPS 32(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X1, X11
	VMULPS  X9, X11, X11
	VMOVAPS X1, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X1

	MOVSD  64(R10), X8
	MOVHPS 80(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X2, X11
	VMULPS  X9, X11, X11
	VMOVAPS X2, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X2

	MOVSD  96(R10), X8
	MOVHPS 120(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X3, X11
	VMULPS  X9, X11, X11
	VMOVAPS X3, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X3

	VMOVUPS X0, 32(R11)
	VMOVUPS X1, 96(R11)
	VMOVUPS X2, 160(R11)
	VMOVUPS X3, 224(R11)

	// Column pairs 6 & 7
	VMOVUPS 48(R9), X0
	VMOVUPS 112(R9), X1
	VMOVUPS 176(R9), X2
	VMOVUPS 240(R9), X3

	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7
	VMOVAPS X4, X0
	VADDPS  X5, X0, X0
	VMOVAPS X4, X2
	VSUBPS  X5, X2, X2
	VMOVAPS X7, X8
	VSHUFPS $0xB1, X8, X8, X8
	VXORPS  X15, X8, X8
	VMOVAPS X6, X1
	VSUBPS  X8, X1, X1
	VMOVAPS X6, X3
	VADDPS  X8, X3, X3

	// Twiddles for col 6, 7
	VMOVUPS 48(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X1, X11
	VMULPS  X9, X11, X11
	VMOVAPS X1, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X1

	MOVSD  96(R10), X8
	MOVHPS 112(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X2, X11
	VMULPS  X9, X11, X11
	VMOVAPS X2, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X2

	MOVSD  144(R10), X8
	MOVHPS 168(R10), X8
	VMOVAPS X8, X9
	VSHUFPS $0xA0, X9, X9, X9
	VMOVAPS X8, X10
	VSHUFPS $0xF5, X10, X10, X10
	VMOVAPS X3, X11
	VMULPS  X9, X11, X11
	VMOVAPS X3, X12
	VSHUFPS $0xB1, X12, X12, X12
	VMULPS  X10, X12, X12
	VADDSUBPS X12, X11, X11
	VMOVAPS X11, X3

	VMOVUPS X0, 48(R11)
	VMOVUPS X1, 112(R11)
	VMOVUPS X2, 176(R11)
	VMOVUPS X3, 240(R11)

	// Step 2: Row FFTs (Size 8)
	VMOVUPS 0(R10), X0
	VMOVUPS 32(R10), X1
	VMOVUPS 64(R10), X2
	VMOVUPS 96(R10), X3

	VMOVAPS X0, X8
	VUNPCKLPD X1, X8, X8
	VMOVAPS X2, X9
	VUNPCKLPD X3, X9, X9

	MOVQ $0, AX
row_loop:
	MOVQ AX, CX
	SHLQ $6, CX
	ADDQ R11, CX
	VMOVUPS 0(CX), X0
	VMOVUPS 16(CX), X1
	VMOVUPS 32(CX), X2
	VMOVUPS 48(CX), X3

	// Stage 1: Sum/Diff (Stride 4)
	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7

	// Unpack Sums
	VMOVAPS X4, X10
	VUNPCKLPD X5, X10, X10
	VMOVAPS X4, X11
	VUNPCKHPD X5, X11, X11

	// Process Sums
	VMOVAPS X10, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X10, X0
	VADDPS  X12, X0, X0
	VMOVAPS X10, X1
	VSUBPS  X12, X1, X1
	VMOVAPS X11, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X11, X2
	VADDPS  X12, X2, X2
	VMOVAPS X11, X3
	VSUBPS  X12, X3, X3

	// Unpack Diffs
	VMOVAPS X6, X10
	VUNPCKLPD X7, X10, X10
	VMOVAPS X6, X11
	VUNPCKHPD X7, X11, X11

	// Process Diffs (Rotated -i)
	VMOVAPS X10, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X12, X13
	VSHUFPS $0xB1, X13, X13, X13
	VMOVUPS ·maskNegHiPS(SB), X15
	VXORPS  X15, X13, X13
	VMOVAPS X10, X4
	VADDPS  X13, X4, X4
	VMOVAPS X10, X5
	VSUBPS  X13, X5, X5

	VMOVAPS X11, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X12, X13
	VSHUFPS $0xB1, X13, X13, X13
	VXORPS  X15, X13, X13
	VMOVAPS X11, X6
	VADDPS  X13, X6, X6
	VMOVAPS X11, X7
	VSUBPS  X13, X7, X7

	// Pack
	VMOVAPS X0, X10
	VUNPCKLPD X4, X10, X10
	VMOVAPS X1, X11
	VUNPCKLPD X5, X11, X11
	VMOVAPS X2, X12
	VUNPCKLPD X6, X12, X12
	VMOVAPS X3, X13
	VUNPCKLPD X7, X13, X13

	// Stage 4: Twiddle
	VMOVAPS X8, X4
	VSHUFPS $0xA0, X4, X4, X4
	VMOVAPS X8, X5
	VSHUFPS $0xF5, X5, X5, X5
	VMOVAPS X12, X6
	VMULPS  X4, X6, X6
	VMOVAPS X12, X7
	VSHUFPS $0xB1, X7, X7, X7
	VMULPS  X5, X7, X7
	VADDSUBPS X7, X6, X6
	VMOVAPS X10, X0
	VADDPS  X6, X0, X0
	VMOVAPS X10, X2
	VSUBPS  X6, X2, X2

	VMOVAPS X9, X4
	VSHUFPS $0xA0, X4, X4, X4
	VMOVAPS X9, X5
	VSHUFPS $0xF5, X5, X5, X5
	VMOVAPS X13, X6
	VMULPS  X4, X6, X6
	VMOVAPS X13, X7
	VSHUFPS $0xB1, X7, X7, X7
	VMULPS  X5, X7, X7
	VADDSUBPS X7, X6, X6
	VMOVAPS X11, X1
	VADDPS  X6, X1, X1
	VMOVAPS X11, X3
	VSUBPS  X6, X3, X3

	// Store transposed to dst
	MOVQ AX, CX
	SHLQ $3, CX
	ADDQ R8, CX

	MOVSD  X0, 0(CX)
	MOVHPS X0, 32(CX)
	MOVSD  X1, 64(CX)
	MOVHPS X1, 96(CX)
	MOVSD  X2, 128(CX)
	MOVHPS X2, 160(CX)
	MOVSD  X3, 192(CX)
	MOVHPS X3, 224(CX)

	INCQ AX
	CMPQ AX, $4
	JL row_loop

	VZEROUPPER
	MOVB $1, ret+120(FP)
	RET

fwd_ret_false:
	VZEROUPPER
	MOVB $0, ret+120(FP)
	RET

// ===========================================================================
// Inverse transform, size 32, complex64, radix-32 (4x8) variant (AVX2 XMM)
// ===========================================================================
TEXT ·InverseAVX2Size32Radix32Complex64Asm(SB), NOSPLIT, $0-121
	MOVQ dst+0(FP), R8
	MOVQ src+24(FP), R9
	MOVQ twiddle+48(FP), R10
	MOVQ scratch+72(FP), R11
	MOVQ bitrev+96(FP), R12
	MOVQ src+32(FP), R13

	CMPQ R13, $32
	JNE  inv_ret_false

	VMOVUPS ·maskNegLoPS(SB), X15
	VMOVUPS ·maskNegHiPS(SB), X14

	// Step 1: Column IFFTs (Size 4)
	MOVQ $0, AX
col_inv_loop:
	MOVQ AX, CX
	SHLQ $4, CX
	MOVQ CX, DX
	ADDQ R9, CX
	VMOVUPS 0(CX), X0
	VMOVUPS 64(CX), X1
	VMOVUPS 128(CX), X2
	VMOVUPS 192(CX), X3

	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7

	VMOVAPS X4, X0
	VADDPS  X5, X0, X0
	VMOVAPS X4, X2
	VSUBPS  X5, X2, X2

	VMOVAPS X7, X8
	VSHUFPS $0xB1, X8, X8, X8
	VXORPS  X15, X8, X8

	VMOVAPS X6, X1
	VADDPS  X8, X1, X1
	VMOVAPS X6, X3
	VSUBPS  X8, X3, X3

	// Twiddles (Conjugated)
	MOVQ AX, CX
	SHLQ $4, CX
	VMOVUPS (R10)(CX*1), X8
	MOVQ CX, DX
	SHLQ $1, DX
	MOVSD  (R10)(DX*1), X9
	MOVHPS 16(R10)(DX*1), X9
	MOVQ CX, DX
	SHLQ $1, DX
	ADDQ CX, DX
	MOVSD  (R10)(DX*1), X10
	MOVHPS 24(R10)(DX*1), X10

	VXORPS X14, X8, X8
	VXORPS X14, X9, X9
	VXORPS X14, X10, X10

	// Apply X8 to X1
	VMOVAPS X8, X11
	VSHUFPS $0xA0, X11, X11, X11
	VMOVAPS X8, X12
	VSHUFPS $0xF5, X12, X12, X12
	VMOVAPS X1, X4
	VMULPS  X11, X4, X4
	VMOVAPS X1, X5
	VSHUFPS $0xB1, X5, X5, X5
	VMULPS  X12, X5, X5
	VADDSUBPS X5, X4, X4
	VMOVAPS X4, X1

	// Apply X9 to X2
	VMOVAPS X9, X11
	VSHUFPS $0xA0, X11, X11, X11
	VMOVAPS X9, X12
	VSHUFPS $0xF5, X12, X12, X12
	VMOVAPS X2, X4
	VMULPS  X11, X4, X4
	VMOVAPS X2, X5
	VSHUFPS $0xB1, X5, X5, X5
	VMULPS  X12, X5, X5
	VADDSUBPS X5, X4, X4
	VMOVAPS X4, X2

	// Apply X10 to X3
	VMOVAPS X10, X11
	VSHUFPS $0xA0, X11, X11, X11
	VMOVAPS X10, X12
	VSHUFPS $0xF5, X12, X12, X12
	VMOVAPS X3, X4
	VMULPS  X11, X4, X4
	VMOVAPS X3, X5
	VSHUFPS $0xB1, X5, X5, X5
	VMULPS  X12, X5, X5
	VADDSUBPS X5, X4, X4
	VMOVAPS X4, X3

	// Store to scratch
	MOVQ AX, CX
	SHLQ $4, CX
	VMOVUPS X0, (R11)(CX*1)
	ADDQ $64, CX
	VMOVUPS X1, (R11)(CX*1)
	ADDQ $64, CX
	VMOVUPS X2, (R11)(CX*1)
	ADDQ $64, CX
	VMOVUPS X3, (R11)(CX*1)

	INCQ AX
	CMPQ AX, $4
	JL col_inv_loop

	// Step 2: Row IFFTs (Size 8)
	VMOVUPS 0(R10), X0
	VMOVUPS 32(R10), X1
	VMOVUPS 64(R10), X2
	VMOVUPS 96(R10), X3
	VMOVAPS X0, X8
	VUNPCKLPD X1, X8, X8
	VMOVAPS X2, X9
	VUNPCKLPD X3, X9, X9
	VXORPS  X14, X8, X8
	VXORPS  X14, X9, X9

	MOVQ $0, AX
row_inv_loop:
	MOVQ AX, CX
	SHLQ $6, CX
	ADDQ R11, CX
	VMOVUPS 0(CX), X0
	VMOVUPS 16(CX), X1
	VMOVUPS 32(CX), X2
	VMOVUPS 48(CX), X3

	// Stage 1
	VMOVAPS X0, X4
	VADDPS  X2, X4, X4
	VMOVAPS X1, X5
	VADDPS  X3, X5, X5
	VMOVAPS X0, X6
	VSUBPS  X2, X6, X6
	VMOVAPS X1, X7
	VSUBPS  X3, X7, X7

	VMOVAPS X4, X10
	VUNPCKLPD X5, X10, X10
	VMOVAPS X4, X11
	VUNPCKHPD X5, X11, X11

	VMOVAPS X10, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X10, X0
	VADDPS  X12, X0, X0
	VMOVAPS X10, X1
	VSUBPS  X12, X1, X1
	VMOVAPS X11, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X11, X2
	VADDPS  X12, X2, X2
	VMOVAPS X11, X3
	VSUBPS  X12, X3, X3

	VMOVAPS X6, X10
	VUNPCKLPD X7, X10, X10
	VMOVAPS X6, X11
	VUNPCKHPD X7, X11, X11

	VMOVAPS X10, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X12, X13
	VSHUFPS $0xB1, X13, X13, X13
	VMOVUPS ·maskNegLoPS(SB), X15
	VXORPS  X15, X13, X13
	VMOVAPS X10, X4
	VADDPS  X13, X4, X4
	VMOVAPS X10, X5
	VSUBPS  X13, X5, X5

	VMOVAPS X11, X12
	VSHUFPS $0x4E, X12, X12, X12
	VMOVAPS X12, X13
	VSHUFPS $0xB1, X13, X13, X13
	VXORPS  X15, X13, X13
	VMOVAPS X11, X6
	VADDPS  X13, X6, X6
	VMOVAPS X11, X7
	VSUBPS  X13, X7, X7

	VMOVAPS X0, X10
	VUNPCKLPD X4, X10, X10
	VMOVAPS X1, X11
	VUNPCKLPD X5, X11, X11
	VMOVAPS X2, X12
	VUNPCKLPD X6, X12, X12
	VMOVAPS X3, X13
	VUNPCKLPD X7, X13, X13

	// Stage 4: Twiddle
	VMOVAPS X8, X4
	VSHUFPS $0xA0, X4, X4, X4
	VMOVAPS X8, X5
	VSHUFPS $0xF5, X5, X5, X5
	VMOVAPS X12, X6
	VMULPS  X4, X6, X6
	VMOVAPS X12, X7
	VSHUFPS $0xB1, X7, X7, X7
	VMULPS  X5, X7, X7
	VADDSUBPS X7, X6, X6
	VMOVAPS X10, X0
	VADDPS  X6, X0, X0
	VMOVAPS X10, X2
	VSUBPS  X6, X2, X2

	VMOVAPS X9, X4
	VSHUFPS $0xA0, X4, X4, X4
	VMOVAPS X9, X5
	VSHUFPS $0xF5, X5, X5, X5
	VMOVAPS X13, X6
	VMULPS  X4, X6, X6
	VMOVAPS X13, X7
	VSHUFPS $0xB1, X7, X7, X7
	VMULPS  X5, X7, X7
	VADDSUBPS X7, X6, X6
	VMOVAPS X11, X1
	VADDPS  X6, X1, X1
	VMOVAPS X11, X3
	VSUBPS  X6, X3, X3

	// Normalization (1/32)
	VMOVSS  ·thirtySecond32(SB), X15
	VSHUFPS $0x00, X15, X15, X15
	VMULPS  X15, X0, X0
	VMULPS  X15, X1, X1
	VMULPS  X15, X2, X2
	VMULPS  X15, X3, X3

	// Store transposed
	MOVQ AX, CX
	SHLQ $3, CX
	ADDQ R8, CX
	MOVSD  X0, 0(CX)
	MOVHPS X0, 32(CX)
	MOVSD  X1, 64(CX)
	MOVHPS X1, 96(CX)
	MOVSD  X2, 128(CX)
	MOVHPS X2, 160(CX)
	MOVSD  X3, 192(CX)
	MOVHPS X3, 224(CX)

	INCQ AX
	CMPQ AX, $4
	JL row_inv_loop

	VZEROUPPER
	MOVB $1, ret+120(FP)
	RET

inv_ret_false:
	VZEROUPPER
	MOVB $0, ret+120(FP)
	RET
